{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAK5G0Dm4qFdQqxzYrSTkw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arjunsumina/analytics_code/blob/main/artificial_intelligence_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------------\n",
        "1. Understanding Numpy ND array\n",
        "\n",
        "1.   Basic Neural Networks\n",
        "2.   Deep Neural Networks\n",
        "\n",
        "---------------------------------\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JNgsnLT7WBWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Understanding Numpy ND array"
      ],
      "metadata": {
        "id": "VCogDtuXPuko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# shape gives the length of the array in each direction\n",
        "# dimension gives the number of dimensions\n",
        "# understanding 1 dimensional numpy nd arrays\n",
        "print('1d array')\n",
        "a_1d = np.arange(3)\n",
        "print(a_1d)\n",
        "print(a_1d.ndim)\n",
        "print(a_1d.shape)\n",
        "print(a_1d.size)\n",
        "# understanding 2 dimensional numpy nd arrays\n",
        "print('2d array')\n",
        "a_2d = np.arange(12).reshape((3, 4))\n",
        "print(a_2d)\n",
        "print(a_2d.ndim)\n",
        "print(a_2d.shape)\n",
        "print(a_2d.size)\n",
        "# understanding 3 dimensional numpy nd arrays\n",
        "print('3d array')\n",
        "a_3d = np.arange(24).reshape((2, 3, 4))\n",
        "print(a_3d)\n",
        "print(a_3d.ndim)\n",
        "print(a_3d.shape)\n",
        "print(a_3d.size)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkMvQJoYZQwE",
        "outputId": "820207ed-3ec9-4979-8b0b-e06971a692ca"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1d array\n",
            "[0 1 2]\n",
            "1\n",
            "(3,)\n",
            "3\n",
            "2d array\n",
            "[[ 0  1  2  3]\n",
            " [ 4  5  6  7]\n",
            " [ 8  9 10 11]]\n",
            "2\n",
            "(3, 4)\n",
            "12\n",
            "3d array\n",
            "[[[ 0  1  2  3]\n",
            "  [ 4  5  6  7]\n",
            "  [ 8  9 10 11]]\n",
            "\n",
            " [[12 13 14 15]\n",
            "  [16 17 18 19]\n",
            "  [20 21 22 23]]]\n",
            "3\n",
            "(2, 3, 4)\n",
            "24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYr9ivhbV87B",
        "outputId": "b3db52f9-355d-434e-eab7-59b1a12c5c44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input x is [[0 0 1]\n",
            " [0 1 1]\n",
            " [1 0 1]\n",
            " [1 1 1]]\n",
            "dimension of x 2\n",
            "shape of x (4, 3)\n",
            "size of x 12\n",
            "output y is [0 0 0 1]\n",
            "dimension of y 1\n",
            "shape of y (4,)\n",
            "size of y 4\n",
            "input w is [0. 0. 0.]\n",
            "dimension of w 1\n",
            "shape of w (3,)\n",
            "size of w 3\n"
          ]
        }
      ],
      "source": [
        "# implementing and gate using perceptron\n",
        "# declare the set of inputs as numpy array\n",
        "# the four sets of two binary inputs are x1,x2\n",
        "# The bias term has been set as 0.\n",
        "# the three weights are 0,0,0\n",
        "# the four outputs are 0,0,0,1\n",
        "import numpy as np\n",
        "x = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
        "print('input x is',x)\n",
        "print('dimension of x',x.ndim)\n",
        "print('shape of x',x.shape)\n",
        "print('size of x',x.size)\n",
        "y = np.array([0, 0, 0, 1])\n",
        "print('output y is',y)\n",
        "print('dimension of y',y.ndim)\n",
        "print('shape of y',y.shape)\n",
        "print('size of y',y.size)\n",
        "w = np.array([0.0, 0.0, 0.0])\n",
        "print('input w is',w)\n",
        "print('dimension of w',w.ndim)\n",
        "print('shape of w',w.shape)\n",
        "print('size of w',w.size)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# heaviside function is the unit step function\n",
        "# if the input to the heaviside function is greater than zero, output is 1\n",
        "# if the input to the heaviside function is lower than zero, output is 0.\n",
        "# if the input to the heaviside function is equal to zero, output can be set to\n",
        "# user requirements\n",
        "# number of iterations = 100\n",
        "# learning rate = .5\n",
        "# for \"number of iterations\", we feed the neural network with the four sets\n",
        "# of inputs and check the difference of the expected ouptut and predicted output\n",
        "# for each input combination, we compute the error and update all the weights\n",
        "# in the neural network.\n",
        "x = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
        "y = np.array([0, 0, 0, 1])\n",
        "w = np.array([0.0, 0.0, 0.0])\n",
        "\n",
        "eta = 0.5\n",
        "print(w)\n",
        "print(y.ndim)\n",
        "for t in range(10000):\n",
        "    for i in range(len(y)):\n",
        "        y_pred = np.heaviside(np.dot(x[i], w), 0)\n",
        "        error = y[i] - y_pred\n",
        "        w +=  error * eta * x[i]\n",
        "        #print(w)\n",
        "\n",
        "print('weight')\n",
        "print(w)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFcyAarneEp2",
        "outputId": "d7e91dfd-e5b2-4d1c-be72-b6755d2c392a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0.]\n",
            "1\n",
            "weight\n",
            "[ 1.   0.5 -1. ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "faster way of implementing an and gate using matrix method\n",
        "in the above case,weight updation was done for each combination for input\n",
        "but now we update the weight only once for every 4 sets of input combinations of the and gate bold\n"
      ],
      "metadata": {
        "id": "ie_fUSptIdGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "x = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
        "y = np.array([0, 0, 0, 1])\n",
        "w = np.array([0.0, 0.0, 0.0])\n",
        "eta = 0.5\n",
        "for t in range(10000):\n",
        "    y_pred = np.heaviside(np.dot(x, w), 0)\n",
        "    w += np.dot((y - y_pred), x)\n",
        "print(w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZS1ObDjrWmLU",
        "outputId": "ef8dc8dd-310b-44e6-c5c3-cadcf6a2fe33"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.  1. -1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nand gate with sigmoid function\n",
        "# defining sigmoid function\n",
        "def sigmoid(v):\n",
        "    return 1.0 / (1 + np.exp(-v))\n",
        "\n",
        "import numpy as np\n",
        "x = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
        "y = np.array([1.0, 1.0, 1.0, 0.0])\n",
        "weight = np.array([0.0, 0.0, 0.0])\n",
        "\n",
        "eta = 0.5\n",
        "for t in range(100):\n",
        "    y_pred = sigmoid(np.dot(x, weight))\n",
        "    sigmoid_derivative = np.dot(y_pred,1-y_pred)\n",
        "    #weight += np.dot((y - y_pred), x)\n",
        "    weight += np.dot(np.dot((y - y_pred), x),sigmoid_derivative)\n",
        "    #print(weight)\n",
        "\n",
        "\n",
        "y_pred = sigmoid(np.dot(x, weight))\n",
        "print(y_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IEV07UbJxI8",
        "outputId": "e8f4e359-ad43-4fe8-ecfa-582edae258a5"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 1.]\n",
            "[-0.36343099 -0.36343099  1.05958575]\n",
            "[-0.58280338 -0.58280338  1.35848403]\n",
            "[-0.77885947 -0.77885947  1.60009815]\n",
            "[-0.94580114 -0.94580114  1.81855174]\n",
            "[-1.09273432 -1.09273432  2.01274185]\n",
            "[-1.22336796 -1.22336796  2.18811038]\n",
            "[-1.34097745 -1.34097745  2.347832  ]\n",
            "[-1.44793474 -1.44793474  2.49449411]\n",
            "[-1.54604105 -1.54604105  2.63010603]\n",
            "[-1.63668352 -1.63668352  2.75625493]\n",
            "[-1.72094882 -1.72094882  2.87421076]\n",
            "[-1.79970196 -1.79970196  2.98500298]\n",
            "[-1.87364173 -1.87364173  3.08947652]\n",
            "[-1.94334028 -1.94334028  3.18833287]\n",
            "[-2.00927178 -2.00927178  3.28216069]\n",
            "[-2.07183358 -2.07183358  3.37145874]\n",
            "[-2.13136197 -2.13136197  3.45665338]\n",
            "[-2.18814414 -2.18814414  3.53811205]\n",
            "[-2.24242737 -2.24242737  3.61615373]\n",
            "[-2.2944262  -2.2944262   3.69105723]\n",
            "[-2.34432798 -2.34432798  3.76306774]\n",
            "[-2.39229738 -2.39229738  3.83240208]\n",
            "[-2.43847993 -2.43847993  3.89925306]\n",
            "[-2.48300495 -2.48300495  3.96379286]\n",
            "[-2.52598789 -2.52598789  4.02617601]\n",
            "[-2.56753227 -2.56753227  4.08654169]\n",
            "[-2.60773132 -2.60773132  4.1450158 ]\n",
            "[-2.6466693  -2.6466693   4.20171258]\n",
            "[-2.68442267 -2.68442267  4.25673605]\n",
            "[-2.72106099 -2.72106099  4.31018122]\n",
            "[-2.75664777 -2.75664777  4.3621351 ]\n",
            "[-2.79124117 -2.79124117  4.41267761]\n",
            "[-2.82489458 -2.82489458  4.46188234]\n",
            "[-2.85765713 -2.85765713  4.50981719]\n",
            "[-2.88957417 -2.88957417  4.55654497]\n",
            "[-2.92068762 -2.92068762  4.60212391]\n",
            "[-2.95103631 -2.95103631  4.64660808]\n",
            "[-2.98065633 -2.98065633  4.69004779]\n",
            "[-3.00958122 -3.00958122  4.73248995]\n",
            "[-3.03784225 -3.03784225  4.77397837]\n",
            "[-3.06546863 -3.06546863  4.81455399]\n",
            "[-3.09248764 -3.09248764  4.85425519]\n",
            "[-3.11892486 -3.11892486  4.89311796]\n",
            "[-3.14480426 -3.14480426  4.93117612]\n",
            "[-3.17014835 -3.17014835  4.96846146]\n",
            "[-3.19497833 -3.19497833  5.00500394]\n",
            "[-3.21931413 -3.21931413  5.0408318 ]\n",
            "[-3.24317455 -3.24317455  5.07597171]\n",
            "[-3.26657735 -3.26657735  5.11044888]\n",
            "[-3.28953931 -3.28953931  5.14428713]\n",
            "[-3.31207628 -3.31207628  5.17750907]\n",
            "[-3.33420331 -3.33420331  5.2101361 ]\n",
            "[-3.35593462 -3.35593462  5.24218854]\n",
            "[-3.37728375 -3.37728375  5.27368568]\n",
            "[-3.39826353 -3.39826353  5.30464588]\n",
            "[-3.41888617 -3.41888617  5.33508659]\n",
            "[-3.43916327 -3.43916327  5.36502443]\n",
            "[-3.4591059  -3.4591059   5.39447525]\n",
            "[-3.47872459 -3.47872459  5.42345416]\n",
            "[-3.49802939 -3.49802939  5.45197558]\n",
            "[-3.51702988 -3.51702988  5.48005329]\n",
            "[-3.53573524 -3.53573524  5.50770045]\n",
            "[-3.55415422 -3.55415422  5.53492966]\n",
            "[-3.57229518 -3.57229518  5.56175296]\n",
            "[-3.59016615 -3.59016615  5.58818192]\n",
            "[-3.6077748  -3.6077748   5.61422758]\n",
            "[-3.62512848 -3.62512848  5.63990055]\n",
            "[-3.64223425 -3.64223425  5.66521102]\n",
            "[-3.65909887 -3.65909887  5.69016875]\n",
            "[-3.67572884 -3.67572884  5.71478312]\n",
            "[-3.69213039 -3.69213039  5.73906316]\n",
            "[-3.70830952 -3.70830952  5.76301752]\n",
            "[-3.724272   -3.724272    5.78665456]\n",
            "[-3.74002336 -3.74002336  5.8099823 ]\n",
            "[-3.75556895 -3.75556895  5.83300848]\n",
            "[-3.77091391 -3.77091391  5.85574053]\n",
            "[-3.78606317 -3.78606317  5.87818563]\n",
            "[-3.80102152 -3.80102152  5.90035071]\n",
            "[-3.81579356 -3.81579356  5.92224244]\n",
            "[-3.83038371 -3.83038371  5.94386727]\n",
            "[-3.84479628 -3.84479628  5.9652314 ]\n",
            "[-3.85903538 -3.85903538  5.98634086]\n",
            "[-3.87310501 -3.87310501  6.00720145]\n",
            "[-3.88700904 -3.88700904  6.02781877]\n",
            "[-3.90075119 -3.90075119  6.04819825]\n",
            "[-3.91433507 -3.91433507  6.06834514]\n",
            "[-3.92776416 -3.92776416  6.08826452]\n",
            "[-3.94104185 -3.94104185  6.1079613 ]\n",
            "[-3.9541714  -3.9541714   6.12744025]\n",
            "[-3.96715598 -3.96715598  6.14670597]\n",
            "[-3.97999864 -3.97999864  6.16576293]\n",
            "[-3.99270236 -3.99270236  6.18461546]\n",
            "[-4.00527003 -4.00527003  6.20326776]\n",
            "[-4.01770442 -4.01770442  6.22172391]\n",
            "[-4.03000825 -4.03000825  6.23998785]\n",
            "[-4.04218415 -4.04218415  6.25806342]\n",
            "[-4.05423466 -4.05423466  6.27595433]\n",
            "[-4.06616226 -4.06616226  6.29366421]\n",
            "[-4.07796935 -4.07796935  6.31119657]\n",
            "[0.99818743 0.9031939  0.9031939  0.13649141]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# or gate with sigmoid function\n",
        "# defining sigmoid function\n",
        "def sigmoid(v):\n",
        "    return 1.0 / (1 + np.exp(-v))\n",
        "\n",
        "import numpy as np\n",
        "x = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
        "y = np.array([0.0, 0.0, 0.0, 1.0])\n",
        "weight = np.array([0.0, 0.0, 0.0])\n",
        "\n",
        "eta = 0.5\n",
        "for t in range(100):\n",
        "    y_pred = sigmoid(np.dot(x, weight))\n",
        "    sigmoid_derivative = np.dot(y_pred,1-y_pred)\n",
        "    #weight += np.dot((y - y_pred), x)\n",
        "    weight += np.dot(np.dot((y - y_pred), x),sigmoid_derivative)\n",
        "    #print(weight)\n",
        "\n",
        "\n",
        "y_pred = sigmoid(np.dot(x, weight))\n",
        "print(y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO44kaOYOWFk",
        "outputId": "3f84f2d5-49a7-435d-a68b-429c22ac3188"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.00181257 0.0968061  0.0968061  0.86350859]\n"
          ]
        }
      ]
    }
  ]
}